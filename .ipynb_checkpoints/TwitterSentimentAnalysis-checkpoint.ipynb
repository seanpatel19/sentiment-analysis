{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4bc20b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Twitter API \"Tweepy\"\n",
    "\n",
    "This project is the pre-cursor to a Sentiment Analysis-enabled RoboAdvisor to be created using AWS Lex, Lambda, Comprehend and SageMaker. Here, the basic Sentiment Analysis functionality has been built, implementing NLTK and TextBlob NLP libraries and the Twitter API, Teepy. This will be expanded to additonally implement IBM Watson NLU, AWS Comprehend and VADER (Valence Aware Dictionary and sEntiment Reasoner), and will scrape text from other sites, such as Reddit and Google Trends. The code for these additonal functionalities has been completed, but has been separated into other notebooks until the next version of the app is developed. The CLI of this app will later become a bot...\n",
    "\n",
    "This code allows one to analyze sentiment around a topic of choice using tweets. PieChart and WordCloud visualizations are generated, as well as ngram models that provide the user with a glance at which words are most used and which words are used together.\n",
    "\n",
    "\n",
    "## Notes on Installations and Libraries:\n",
    "\n",
    "Tweepy, the Twitter API supports both OAuth 1a (application-user) and OAuth 2 (application-only) authentication. Authentication is handled by the tweepy.AuthHandler class. OAuth 2 is a method of authentication where an application makes API requests without the user context. Use this method if you just need read-only access to public information.\n",
    "\n",
    "**You need to have a Twitter Developer Account before authorizations!**\n",
    "\n",
    "**TextBlob** is a Python (2 and 3) library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more.\n",
    "\n",
    "**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "\n",
    "## What is Sentiment Analysis?\n",
    "\n",
    "Sentiment Analysis is the process of computationally determining whether a piece of writing is positive, negative, or neutral. It is also known as **\"opinion mining\"**, deriving the opinion or attitude of a speaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc9c4d",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912f41f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tweepy in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.11.1 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bruhhhhhh\\anaconda3\\envs\\dev\\lib\\site-packages (from requests-oauthlib<2,>=1.0.0->tweepy) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install Libraries\n",
    "!pip install textblob\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d549d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1ce05445d243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt #to display our wordcloud\n",
    "import pandas as pd\n",
    "import numpy as np #to get the color of our image\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image #to load out image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dacfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb091d9",
   "metadata": {},
   "source": [
    "## Tweepy Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce3c39c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Consumer key must be string or bytes, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2ca062711fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccessTokenSecret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TWITTER_ACCESS_TOKEN_SECRET\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconsumerKey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumerSecret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccessToken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccessTokenSecret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\tweepy\\auth.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, consumer_key, consumer_secret, callback)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             raise TypeError(\"Consumer key must be string or bytes, not \"\n\u001b[1;32m---> 37\u001b[1;33m                             + type(consumer_key).__name__)\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconsumer_secret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             raise TypeError(\"Consumer secret must be string or bytes, not \"\n",
      "\u001b[1;31mTypeError\u001b[0m: Consumer key must be string or bytes, not NoneType"
     ]
    }
   ],
   "source": [
    "# Authentication\n",
    "consumerKey = os.getenv(\"TWITTER_API_KEY\")\n",
    "consumerSecret = os.getenv(\"TWITTER_SECRET_KEY\")\n",
    "accessToken = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "accessTokenSecret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de76cba",
   "metadata": {},
   "source": [
    "## Fetching Tweets by Hashtag or Keyword\n",
    "\n",
    "Use **Tweepy** to fetch text (tweets), then use TextBlob to calculate positive, negative, neutral, polarity and compound parameters from the text.\n",
    "The user types into the CLI the keyword or hashtag and the number of tweets they want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up, basic Sentiment Analysis review\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole) \n",
    "\n",
    "keyword = input(\"Please enter keyword or hashtag to search: \")\n",
    "noOfTweet = int(input (\"Please enter how many tweets to analyze: \"))\n",
    "\n",
    "\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)\n",
    "positive  = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "    \n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    \n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529a96c",
   "metadata": {},
   "source": [
    "### Viewing a breakdown of the tweets by Sentiment Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85794096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print(\"total number: \",len(tweet_list))\n",
    "print(\"positive number: \",len(positive_list))\n",
    "print(\"negative number: \", len(negative_list))\n",
    "print(\"neutral number: \",len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the Tweets DataFrame\n",
    "tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bde78",
   "metadata": {},
   "source": [
    "### Review the Sentiment Categories breakdown in PieChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword=  \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed666b19",
   "metadata": {},
   "source": [
    "## Cleaning Tweets before further Sentiment Analysis\n",
    "\n",
    "1. Remove duplicated tweets\n",
    "2. Create a new DataFrame (tw_list) and a new feature (text), then clean text of RTs, links, and punctuation using lambda function\n",
    "3. Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the drop_duplicates function to remove dupicate tweets from the list.\n",
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the list of tweets, noting the number of duplicated tweets removed.\n",
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new DataFrame and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "\n",
    "# Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2452449",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39706d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ca5af",
   "metadata": {},
   "source": [
    "### Sentiment-Based Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6246ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for count_values_in single columns\n",
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb461da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a470152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for Pie Chart\n",
    "pc = count_values_in_column(tw_list,\"sentiment\")\n",
    "names= pc.index\n",
    "size=pc[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48930a72",
   "metadata": {},
   "source": [
    "### Visualization Sentiments with WordCloud\n",
    "\n",
    "This helps us get an understanding of which words are used most in the tweets we are analyzing.\n",
    "\n",
    "Define a WordCloud function to be used several times (for all tweets, positive tweets, negative tweets, and neutral tweets). Save the WordCloud image to file for later upload to GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"blk_dollar.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "                  mask = mask,\n",
    "                  max_words=350,\n",
    "                  stopwords=stopwords,\n",
    "                  repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for all tweets\n",
    "create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d67f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc387c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wordcloud for neutral sentiment\n",
    "create_wordcloud(tw_list_neutral[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d167029",
   "metadata": {},
   "source": [
    "### Calculate Tweet Length and Wordcount \n",
    "\n",
    "This helps us gain an understanding of the density of words and characters used in tweets based on different sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1867ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating tweet's length and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44968918",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a947d8",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe80cc6",
   "metadata": {},
   "source": [
    "**\"Tokenization\"** involves splitting sentences and words from the body of the text. One can think of \"tokens\" as parts, like a word is a \"token\" in a sentence, and a sentence is a \"token\" in a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb81f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b488c",
   "metadata": {},
   "source": [
    "The process of converting data to something a computer an understand is referred to as pre-processing. One of the major form of pre-processing is to filter out useless data. In natural language processing (NLP), useless words (data) are referred to as **stopwords**.\n",
    "\n",
    "A **stopword** is a commonly used word (such as \"a,\" \"an,\" \"the,\" \"in\") that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. We would not want these words to take up space in our database or take up valuable processing time. So, we remove them by storing a list of words that you consider as stopwords. Here, we use **NLTK** (Natural Language ToolKit), which has a list of stopwords in 16 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8782a0f",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of producing morphological variants of a root word. A stemming algorithm, for example, reduces the words \"chocolates\", \"chocolatey\", \"choco\" to the root word \"chocolate\", and \"retrieval\", \"retrieves\", \"retrieved\" reduce to the stem \"retrieve.\" This allows us to consider all words that come from the same root word as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25e998",
   "metadata": {},
   "source": [
    "~ Test Out: Lemmatization ~\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming, but it brings context to the words, linking words with similar meaning to one word.\n",
    "\n",
    "side note: I read that lemmatization is actually preferred over stemming lemmatization does morphological analysis of the words... try it out later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93664970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd1d02",
   "metadata": {},
   "source": [
    "### Using CountVectorizer to Extract Features from Text\n",
    "\n",
    "CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple texts and we wish to convert each word in each text into vectors for use in further text analysis.\n",
    "\n",
    "CountVectorizer creates a matrix where each unique word is represented by a column in the matrix and each text sample is a row. The value of each cell is nothing but the count of the word in that particular text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying CountVectorizer to see all unique words as new features\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('The {} Tweets analyzed have {} unique words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3ba92",
   "metadata": {},
   "source": [
    "### Most Used Words\n",
    "\n",
    "Use the sort_values function and organize in descending order to see which words are used the most in the Tweets analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d48943",
   "metadata": {},
   "source": [
    "### Building N-gram Language Models\n",
    "\n",
    "**Language Modelling** is a process of determining the probability of any sequence of words. **N-gram modelling** is an example of **Statistical Language Modelling**, the development of probabilistic models that are able to predict the next word in the sequence given the words that precede. An alternative to this is Neural Language Modelling which uses neural network methods (an example of this is word embeddings).\n",
    "\n",
    "* **N-gram** can be defined as the contiguous sequence of n items from a given sample of text or speech. The items can be letters, words, or base pairs according to the application. The n-grams are typically collected from a text or speech corpus (a long dataset).\n",
    "\n",
    "This helps us predict the most probable words that might follow a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7825ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
    "\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b0673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81dd79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e53704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
